#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import json
import argparse
import requests
import dns.resolver
from concurrent.futures import ThreadPoolExecutor, as_completed

URLS_TXT = "urls.txt"
TMP_DIR = "tmp"
DIST_DIR = "dist"
MERGED_FILE = "merged_rules.txt"
PARTS = 16
DNS_BATCH_SIZE = 50

FAIL_DB_FILE = "fails.json"

def load_fail_db():
    if not os.path.exists(FAIL_DB_FILE):
        return {}
    try:
        with open(FAIL_DB_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {}

def save_fail_db(db):
    with open(FAIL_DB_FILE, "w", encoding="utf-8") as f:
        json.dump(db, f, indent=2)

fail_db = load_fail_db()

def download_sources():
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½æ‰€æœ‰è§„åˆ™æº...")
    if not os.path.exists(URLS_TXT):
        print(f"âŒ æ‰¾ä¸åˆ° {URLS_TXT}, è¯·ç¡®è®¤æ–‡ä»¶å­˜åœ¨")
        return

    os.makedirs(TMP_DIR, exist_ok=True)
    with open(URLS_TXT, "r", encoding="utf-8") as f:
        urls = [x.strip() for x in f if x.strip()]

    rules = []
    for url in urls:
        try:
            print(f"Downloading {url}")
            text = requests.get(url, timeout=20).text
            for line in text.splitlines():
                line = line.strip()
                if line and not line.startswith("#"):
                    rules.append(line)
        except:
            print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{url}")

    print(f"âœ… ä¸‹è½½å®Œæˆï¼Œå…± {len(rules)} æ¡è§„åˆ™")
    with open(MERGED_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(rules))
    print("âœ… å·²ç”Ÿæˆ merged_rules.txt")

def split_parts():
    os.makedirs(TMP_DIR, exist_ok=True)
    if not os.path.exists(MERGED_FILE):
        print("âŒ æ²¡æœ‰ merged_rules.txtï¼Œæ— æ³•åˆ‡ç‰‡")
        return

    with open(MERGED_FILE, "r", encoding="utf-8") as f:
        lines = [x.strip() for x in f if x.strip()]

    total = len(lines)
    size = total // PARTS + 1

    print(f"âœ‚ åˆ‡ç‰‡è§„åˆ™ï¼Œå…± {total} æ¡ï¼Œæ¯ç‰‡çº¦ {size} æ¡")

    for i in range(PARTS):
        p = lines[i * size:(i + 1) * size]
        part_file = f"{TMP_DIR}/part_{i+1:02d}.txt"
        with open(part_file, "w", encoding="utf-8") as f:
            f.write("\n".join(p))
        print(f"âœ… {part_file} å…± {len(p)} æ¡")

def dns_lookup(rule):
    try:
        dns.resolver.resolve(rule, "A", lifetime=2)
        return True
    except:
        return False

def handle_dns_result(rule, success):
    rule = rule.strip()

    if success:
        if rule in fail_db:
            del fail_db[rule]
            save_fail_db(fail_db)
        return "ok"

    # è¿ç»­å¤±è´¥è®¡æ•° +1
    fail_db[rule] = fail_db.get(rule, 0) + 1
    save_fail_db(fail_db)

    if fail_db[rule] < 4:
        print(f"âš  {rule} ç¬¬ {fail_db[rule]} æ¬¡å¤±è´¥ï¼ˆæœªåˆ é™¤ï¼‰")
        return "keep"

    # è¿ç»­å¤±è´¥ â‰¥4 â†’ åˆ é™¤
    print(f"âŒ {rule} è¿ç»­å¤±è´¥ {fail_db[rule]} æ¬¡ â†’ å·²åˆ é™¤")
    del fail_db[rule]
    save_fail_db(fail_db)
    return "delete"

def validate_part(part_id):
    os.makedirs(DIST_DIR, exist_ok=True)

    part_file = f"{TMP_DIR}/part_{part_id:02d}.txt"
    if not os.path.exists(part_file):
        print(f"âŒ åˆ†ç‰‡ä¸å­˜åœ¨ï¼š{part_file}")
        return

    with open(part_file, "r", encoding="utf-8") as f:
        rules = [x.strip() for x in f if x.strip()]

    print(f"â± å¼€å§‹éªŒè¯åˆ†ç‰‡ {part_id}ï¼Œå…± {len(rules)} æ¡è§„åˆ™")
    print(f"ğŸš€ å¯åŠ¨ {DNS_BATCH_SIZE} å¹¶å‘éªŒè¯")

    valid_rules = []
    with ThreadPoolExecutor(max_workers=DNS_BATCH_SIZE) as executor:
        future_map = {executor.submit(dns_lookup, rule): rule for rule in rules}

        for future in as_completed(future_map):
            rule = future_map[future]
            try:
                success = future.result()
                res = handle_dns_result(rule, success)
                if res in ("ok", "keep"):
                    valid_rules.append(rule)
            except:
                pass

    outfile = f"{DIST_DIR}/validated_part_{part_id:02d}.txt"
    with open(outfile, "w", encoding="utf-8") as f:
        f.write("\n".join(valid_rules))

    print(f"âœ… åˆ†ç‰‡éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆ {len(valid_rules)} æ¡ â†’ {outfile}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--force-update", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½è§„åˆ™æºå¹¶åˆ‡ç‰‡")
    parser.add_argument("--part", type=int, help="æŒ‡å®šåˆ†ç‰‡éªŒè¯")
    args = parser.parse_args()

    # âœ… é¦–æ¬¡è¿è¡Œæˆ– --force-update â†’ ä¸‹è½½ & åˆ‡ç‰‡
    need_setup = args.force_update or not os.path.exists(MERGED_FILE)

    if need_setup:
        download_sources()
        split_parts()

    # âœ… æŒ‡å®šåˆ†ç‰‡
    if args.part:
        pf = f"{TMP_DIR}/part_{args.part:02d}.txt"
        if not os.path.exists(pf):
            print(f"âš  ç¼ºå°‘åˆ†ç‰‡ {pf}ï¼Œè‡ªåŠ¨é‡æ–°ä¸‹è½½å¹¶åˆ‡ç‰‡")
            download_sources()
            split_parts()

        validate_part(args.part)
