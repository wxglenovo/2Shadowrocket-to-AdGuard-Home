import os
import requests
import dns.resolver
import concurrent.futures
from datetime import datetime
import argparse
import time

URLS_FILE = "urls.txt"
OUTPUT_DIR = "dist"
PARTS = 16
MAX_WORKERS = 80
DNS_BATCH_SIZE = 200  # æ¯æ‰¹å¤„ç†200æ¡ DNS

resolver = dns.resolver.Resolver()
resolver.timeout = 1.5
resolver.lifetime = 1.5
resolver.nameservers = ["1.1.1.1", "8.8.8.8", "9.9.9.9"]

def safe_fetch(url):
    try:
        print(f"ğŸ“¥ ä¸‹è½½ï¼š{url}")
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        return r.text.splitlines()
    except:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼š{url}")
        return []

def clean_rule(line):
    l = line.strip()
    if not l or l.startswith("#") or l.startswith("!") or l.startswith("||browser.events") or l.startswith("||cf.iadsdk") \
       or l.startswith("||dig.bdurl") or l.startswith("||lf-static") or l.startswith("||rt.applovin") or l.startswith("||*.ip6.arpa"):
        return None
    return l

def extract_domain(rule):
    d = rule.lstrip("|").lstrip(".").split("^")[0]
    return d.strip()

def is_valid_domain(domain):
    try:
        resolver.resolve(domain, "A")
        return True
    except:
        return False

def check_batch(rules):
    valid = []
    for rule in rules:
        domain = extract_domain(rule)
        if is_valid_domain(domain):
            valid.append(rule)
    return valid

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--part", type=int, help="æ‰‹åŠ¨æŒ‡å®šåˆ†ç‰‡ 0~15")
    args = parser.parse_args()
    manual_part = args.part

    if not os.path.exists(URLS_FILE):
        print("âŒ æœªæ‰¾åˆ° urls.txt")
        return

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    part_files = [os.path.join(OUTPUT_DIR, f"part_{i}.txt") for i in range(PARTS)]
    valid_output = os.path.join(OUTPUT_DIR, "blocklist_valid.txt")

    # é¦–æ¬¡è¿è¡Œï¼Œä¸‹è½½åˆ‡ç‰‡
    if not os.path.exists(part_files[0]):
        print("ğŸ§© é¦–æ¬¡è¿è¡Œï¼šä¸‹è½½å¹¶åˆ‡ç‰‡")
        with open(URLS_FILE, "r", encoding="utf-8") as f:
            urls = [x.strip() for x in f if x.strip() and not x.startswith("#")]

        all_rules = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as ex:
            for lines in ex.map(safe_fetch, urls):
                all_rules.extend(lines)

        cleaned = list(dict.fromkeys([clean_rule(x) for x in all_rules if clean_rule(x)]))
        total = len(cleaned)
        print(f"âœ… å»é‡åæ€»è®¡ï¼š{total:,} æ¡")

        chunk = total // PARTS
        for idx in range(PARTS):
            start = idx * chunk
            end = None if idx == PARTS - 1 else (idx + 1) * chunk
            with open(part_files[idx], "w", encoding="utf-8") as f:
                f.write("\n".join(cleaned[start:end]))
        print(f"âœ… åˆ‡æˆ {PARTS} ä»½ï¼Œæ¯ä»½çº¦ {chunk:,} æ¡")
        return

    # ç¡®å®šå½“å‰åˆ†ç‰‡
    if manual_part is not None:
        part_index = manual_part
        print(f"â± æ‰‹åŠ¨éªŒè¯åˆ†ç‰‡ï¼š{part_index}")
    else:
        minute = datetime.utcnow().hour * 60 + datetime.utcnow().minute
        part_index = (minute // 25) % PARTS
        print(f"â± è‡ªåŠ¨è½®æ›¿éªŒè¯åˆ†ç‰‡ï¼š{part_index}")

    target_file = part_files[part_index]
    if not os.path.exists(target_file):
        print("âš ï¸ åˆ†ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return

    with open(target_file, "r", encoding="utf-8") as f:
        rules = f.read().splitlines()

    print(f"ğŸ” å½“å‰åˆ†ç‰‡è§„åˆ™ï¼š{len(rules):,} æ¡")

    valid = []
    for i in range(0, len(rules), DNS_BATCH_SIZE):
        batch = rules[i:i+DNS_BATCH_SIZE]
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            results = ex.map(lambda r: r if is_valid_domain(extract_domain(r)) else None, batch)
            valid.extend([r for r in results if r])

    with open(valid_output, "a", encoding="utf-8") as f:
        f.write("\n".join(valid) + "\n")

    print(f"âœ… æœ¬æ¬¡æœ‰æ•ˆï¼š{len(valid):,} æ¡ â†’ å·²è¿½åŠ è‡³ {valid_output}")
    print("ğŸ¯ æ‰§è¡Œç»“æŸï¼Œ0 é”™è¯¯ âœ…")

if __name__ == "__main__":
    main()
