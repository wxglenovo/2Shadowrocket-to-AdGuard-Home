#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import json
import requests
import argparse
import dns.resolver
from concurrent.futures import ThreadPoolExecutor, as_completed

URLS_FILE = "urls.txt"
TMP_DIR = "tmp"
DIST_DIR = "dist"
MERGED_FILE = "merged_rules.txt"
DELETE_COUNTER_FILE = os.path.join(DIST_DIR, "delete_counter.json")
PARTS = 16
DNS_WORKERS = int(os.environ.get("DNS_WORKERS", 50))
DNS_TIMEOUT = 2
DELETE_THRESHOLD = 4

os.makedirs(TMP_DIR, exist_ok=True)
os.makedirs(DIST_DIR, exist_ok=True)


def load_urls():
    if not os.path.exists(URLS_FILE):
        print("âŒ urls.txt ä¸å­˜åœ¨")
        exit(1)
    return [l.strip() for l in open(URLS_FILE, "r", encoding="utf-8") if l.strip()]


def download_and_merge():
    urls = load_urls()
    merged = []
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½è§„åˆ™æºâ€¦")
    for u in urls:
        try:
            r = requests.get(u, timeout=20)
            if r.status_code == 200:
                for line in r.text.splitlines():
                    line = line.strip()
                    if line and not line.startswith("!"):
                        merged.append(line)
                print(f"âœ… è¯»å–: {u}")
            else:
                print(f"âš  æ— æ³•è®¿é—® {u}")
        except:
            print(f"âš  è¯·æ±‚å¤±è´¥: {u}")

    merged = sorted(set(merged))
    with open(MERGED_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(merged))

    print(f"âœ… åˆå¹¶å®Œæˆ: {len(merged)} æ¡è§„åˆ™")
    return merged


def split_to_tmp():
    if not os.path.exists(MERGED_FILE):
        download_and_merge()

    rules = [l.strip() for l in open(MERGED_FILE, "r", encoding="utf-8") if l.strip()]
    total = len(rules)
    per = (total + PARTS - 1) // PARTS

    print(f"ğŸª“ åˆ†ç‰‡ {total} æ¡ï¼Œæ¯ç‰‡çº¦ {per}")

    for i in range(PARTS):
        part_rules = rules[i * per:(i + 1) * per]
        path = os.path.join(TMP_DIR, f"part_{i+1:02d}.txt")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(part_rules))
        print(f"âœ… ç”Ÿæˆ {path} : {len(part_rules)} æ¡")

    return True


def ensure_tmp_exists():
    for i in range(1, PARTS + 1):
        f = os.path.join(TMP_DIR, f"part_{i:02d}.txt")
        if not os.path.exists(f):
            print(f"âš  {f} ä¸å­˜åœ¨ â†’ é‡æ–°ç”Ÿæˆå…¨éƒ¨åˆ†ç‰‡")
            split_to_tmp()
            break


def dns_check(rule):
    domain = rule.replace("||", "").replace("^", "").replace("*", "")
    if not domain:
        return False
    try:
        resolver = dns.resolver.Resolver()
        resolver.timeout = DNS_TIMEOUT
        resolver.lifetime = DNS_TIMEOUT
        resolver.resolve(domain)
        return True
    except:
        return False


def load_delete_counter():
    if os.path.exists(DELETE_COUNTER_FILE):
        try:
            return json.load(open(DELETE_COUNTER_FILE, "r", encoding="utf-8"))
        except:
            print("âš  delete_counter.json æŸåï¼Œé‡ç½®")
    return {}


def save_delete_counter(d):
    json.dump(d, open(DELETE_COUNTER_FILE, "w", encoding="utf-8"), indent=2, ensure_ascii=False)


def validate_part(idx):
    part_path = os.path.join(TMP_DIR, f"part_{idx:02d}.txt")
    if not os.path.exists(part_path):
        print(f"âš  åˆ†ç‰‡ç¼ºå¤± â†’ è‡ªåŠ¨é‡å»º")
        split_to_tmp()

    rules = [l.strip() for l in open(part_path, "r", encoding="utf-8") if l.strip()]

    out_path = os.path.join(DIST_DIR, f"validated_part_{idx:02d}.txt")
    old = set()
    if os.path.exists(out_path):
        old = {l.strip() for l in open(out_path, "r", encoding="utf-8") if l.strip()}

    delete_counter = load_delete_counter()
    kept = set()
    removed = 0
    added = 0

    print(f"ğŸš€ å¹¶å‘éªŒè¯åˆ†ç‰‡ {idx}, å…± {len(rules)} æ¡")

    with ThreadPoolExecutor(max_workers=DNS_WORKERS) as e:
        futures = {e.submit(dns_check, r): r for r in rules}
        done = 0
        for fut in as_completed(futures):
            done += 1
            rule = futures[fut]
            ok = fut.result()

            if ok:
                kept.add(rule)
                delete_counter[rule] = 0
            else:
                cnt = delete_counter.get(rule, 0) + 1
                delete_counter[rule] = cnt
                if cnt < DELETE_THRESHOLD:
                    kept.add(rule)
                else:
                    removed += 1

            if done % 500 == 0:
                print(f"âœ… {done}/{len(rules)} å·²éªŒè¯ï¼Œæœ‰æ•ˆ {len(kept)}")

    for r in kept:
        if r not in old:
            added += 1

    save_delete_counter(delete_counter)

    open(out_path, "w", encoding="utf-8").write("\n".join(sorted(kept)))

    print(f"âœ… åˆ†ç‰‡ {idx} å®Œæˆ: æ€» {len(kept)}, æ–°å¢ {added}, åˆ é™¤ {removed}")
    print(f"COMMIT_STATS: æ€» {len(kept)}, æ–°å¢ {added}, åˆ é™¤ {removed}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--part", type=int)
    args = parser.parse_args()

    if not os.path.exists(MERGED_FILE):
        download_and_merge()

    ensure_tmp_exists()

    if args.part:
        validate_part(args.part)
