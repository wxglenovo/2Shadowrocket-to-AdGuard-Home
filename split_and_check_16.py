#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import json
import requests
import dns.resolver
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# -----------------------------
# é…ç½®
# -----------------------------
URLS_FILE = "urls.txt"
TMP_DIR = "tmp"
DIST_DIR = "dist"
MERGED_FILE = "merged_rules.txt"
DELETE_COUNTER_FILE = os.path.join(DIST_DIR, "delete_counter.json")
PARTS = 16

DNS_WORKERS = int(os.environ.get("DNS_WORKERS", 50))
DNS_TIMEOUT = 3
DELETE_THRESHOLD = 4

os.makedirs(TMP_DIR, exist_ok=True)
os.makedirs(DIST_DIR, exist_ok=True)

# -----------------------------
# ä¸‹è½½å¹¶åˆå¹¶
# -----------------------------
def load_urls():
    if not os.path.exists(URLS_FILE):
        print(f"âŒ {URLS_FILE} ä¸å­˜åœ¨")
        exit(1)
    return [i.strip() for i in open(URLS_FILE, "r", encoding="utf-8") if i.strip()]

def download_and_merge():
    print("ğŸ“¥ ä¸‹è½½ä¸åˆå¹¶è§„åˆ™æº...")
    urls = load_urls()
    merged = set()
    for u in urls:
        try:
            r = requests.get(u, timeout=20)
            r.raise_for_status()
            for line in r.text.splitlines():
                line = line.strip()
                if line and not line.startswith("!"):
                    merged.add(line)
            print(f"âœ… è·å–æˆåŠŸ: {u}")
        except Exception as e:
            print(f"âš  ä¸‹è½½å¤±è´¥: {u} â†’ {e}")

    merged = sorted(merged)
    with open(MERGED_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(merged))
    print(f"âœ… åˆå¹¶å®Œæˆï¼Œå…± {len(merged)} æ¡è§„åˆ™")
    return merged

# -----------------------------
# å†™åˆ†ç‰‡ â†’ tmp/
# -----------------------------
def split_tmp():
    if not os.path.exists(MERGED_FILE):
        download_and_merge()

    rules = [i.strip() for i in open(MERGED_FILE, "r", encoding="utf-8") if i.strip()]

    total = len(rules)
    per = (total + PARTS - 1) // PARTS
    print(f"ğŸª“ å¼€å§‹å†™å…¥ tmp/ åˆ†ç‰‡ï¼Œæ¯ç‰‡çº¦ {per} æ¡")

    for i in range(PARTS):
        part_rules = rules[i * per : (i + 1) * per]
        path = os.path.join(TMP_DIR, f"part_{i+1:02d}.txt")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(part_rules))
        print(f"ğŸ“„ tmp/part_{i+1:02d}.txt â†’ {len(part_rules)} æ¡")
    return True

# -----------------------------
# å†™åˆå§‹ validated_part â†’ dist/
# -----------------------------
def init_dist_from_tmp():
    for i in range(1, PARTS + 1):
        tmp_file = os.path.join(TMP_DIR, f"part_{i:02d}.txt")
        dist_file = os.path.join(DIST_DIR, f"validated_part_{i:02d}.txt")

        if not os.path.exists(tmp_file):
            continue

        if not os.path.exists(dist_file):
            data = open(tmp_file, "r", encoding="utf-8").read()
            with open(dist_file, "w", encoding="utf-8") as f:
                f.write(data)
            print(f"âœ… åˆå§‹å†™å…¥ {dist_file}")

# -----------------------------
# åˆ é™¤è®¡æ•°
# -----------------------------
def load_delete_counter():
    if os.path.exists(DELETE_COUNTER_FILE):
        return json.load(open(DELETE_COUNTER_FILE, "r", encoding="utf-8"))
    return {}

def save_delete_counter(data):
    json.dump(data, open(DELETE_COUNTER_FILE, "w", encoding="utf-8"), indent=2)

# -----------------------------
# DNS è§£æ
# -----------------------------
def dns_valid(rule):
    domain = rule.replace("||", "").replace("^", "").replace("*", "")
    if not domain:
        return False
    try:
        dns.resolver.resolve(domain, "A", lifetime=DNS_TIMEOUT)
        return True
    except:
        return False

# -----------------------------
# éªŒè¯åˆ†ç‰‡
# -----------------------------
def validate_part(n):
    part_file = os.path.join(DIST_DIR, f"validated_part_{n:02d}.txt")
    if not os.path.exists(part_file):
        print(f"âŒ åˆ†ç‰‡ä¸å­˜åœ¨: {part_file}")
        return 0, 0, 0

    rules = [i.strip() for i in open(part_file, "r", encoding="utf-8") if i.strip()]
    print(f"ğŸš€ åˆ†ç‰‡ {n:02d} å¼€å§‹éªŒè¯ï¼Œå…± {len(rules)} æ¡")

    delete_counter = load_delete_counter()
    keep, remove = [], []

    with ThreadPoolExecutor(max_workers=DNS_WORKERS) as pool:
        futures = {pool.submit(dns_valid, r): r for r in rules}
        done = 0
        for future in as_completed(futures):
            rule = futures[future]
            ok = future.result()
            done += 1
            if done % 500 == 0 or done == len(rules):
                print(f"âœ… å·²éªŒè¯ {done}/{len(rules)}")

            if ok:
                keep.append(rule)
                delete_counter.pop(rule, None)
            else:
                delete_counter[rule] = delete_counter.get(rule, 0) + 1
                if delete_counter[rule] >= DELETE_THRESHOLD:
                    remove.append(rule)
                else:
                    keep.append(rule)
                    print(f"âš  {rule} è¿ç»­å¤±è´¥ {delete_counter[rule]}/{DELETE_THRESHOLD}")

    save_delete_counter(delete_counter)

    with open(part_file, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(keep)))

    print(f"âœ… åˆ†ç‰‡ {n:02d} â†’ ä¿ç•™ {len(keep)}, åˆ é™¤ {len(remove)}")
    return len(rules), len(keep), len(remove)

# -----------------------------
# å…¥å£
# -----------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--part", type=int, help="éªŒè¯æŒ‡å®šåˆ†ç‰‡ 1~16")
    parser.add_argument("--force-update", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½ä¸åˆ‡ç‰‡")
    args = parser.parse_args()

    # è‹¥ç¼ºå¤± merged æˆ– tmp åˆ†ç‰‡ â†’ è‡ªåŠ¨ç”Ÿæˆ
    if args.force_update or not os.path.exists(MERGED_FILE) or not os.path.exists(os.path.join(TMP_DIR, "part_01.txt")):
        download_and_merge()
        split_tmp()

    init_dist_from_tmp()

    parts = [args.part] if args.part else list(range(1, PARTS + 1))

    total = kept = removed = 0
    for p in parts:
        t, k, r = validate_part(p)
        total += t
        kept += k
        removed += r

    print(f"COMMIT_STATS: æ€» {total}, æœ‰æ•ˆ {kept}, åˆ é™¤ {removed}")
