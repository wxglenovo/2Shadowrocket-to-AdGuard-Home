#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import requests
import argparse
import dns.resolver
import json

URLS_TXT = "urls.txt"
TMP_DIR = "tmp"
DIST_DIR = "dist"
MERGED_RULE = "merged_rules.txt"
PARTS = 16
DNS_BATCH_SIZE = 50
DELETE_CONFIRM_TIMES = 4
DELETE_COUNTER_FILE = "delete_counter.json"

def load_delete_counter():
    if os.path.exists(DELETE_COUNTER_FILE):
        with open(DELETE_COUNTER_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_delete_counter(counter):
    with open(DELETE_COUNTER_FILE, "w", encoding="utf-8") as f:
        json.dump(counter, f, ensure_ascii=False, indent=2)

def download_rules():
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½è§„åˆ™æº...")
    if not os.path.exists(URLS_TXT):
        print(f"âŒ æ‰¾ä¸åˆ° {URLS_TXT}")
        return []

    rules = []
    with open(URLS_TXT, "r", encoding="utf-8") as f:
        for url in f:
            url = url.strip()
            if not url:
                continue
            print(f"Downloading {url}")
            try:
                r = requests.get(url, timeout=15)
                if r.status_code == 200:
                    for line in r.text.splitlines():
                        line = line.strip()
                        if line and not line.startswith("#"):
                            rules.append(line)
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{url} - {e}")
    print(f"âœ… ä¸‹è½½å®Œæˆï¼Œæ€»è§„åˆ™æ•°: {len(rules)}")
    return rules

def merge_rules():
    os.makedirs(TMP_DIR, exist_ok=True)
    os.makedirs(DIST_DIR, exist_ok=True)

    rules = download_rules()
    rules = sorted(set(rules))

    with open(MERGED_RULE, "w", encoding="utf-8") as f:
        f.write("\n".join(rules))

    print(f"âœ… åˆå¹¶å®Œæˆ: {len(rules)} æ¡è§„åˆ™")

    size = len(rules)
    part_len = size // PARTS + 1

    for i in range(PARTS):
        part = rules[i * part_len:(i + 1) * part_len]
        part_file = os.path.join(TMP_DIR, f"part_{i+1:02d}.txt")
        with open(part_file, "w", encoding="utf-8") as f:
            f.write("\n".join(part))
        print(f"ğŸ“¦ åˆ†ç‰‡ {i+1:02d} â†’ {len(part)} æ¡")

def dns_check(domain):
    resolver = dns.resolver.Resolver()
    resolver.timeout = 2
    resolver.lifetime = 2
    try:
        resolver.resolve(domain)
        return True
    except:
        return False

def validate_rules(rules):
    valid = []
    for rule in rules:
        d = rule.replace("||", "").replace("^", "")
        if "." not in d:
            continue
        if dns_check(d):
            valid.append(rule)
    return valid

def process_part(part):
    part_file = os.path.join(TMP_DIR, f"part_{part:02d}.txt")
    if not os.path.exists(part_file):
        print(f"âŒ åˆ†ç‰‡ä¸å­˜åœ¨: {part_file}")
        return

    with open(part_file, "r", encoding="utf-8") as f:
        rules = [i.strip() for i in f if i.strip()]

    total_before = len(rules)
    print(f"â³ åˆ†ç‰‡ {part:02d} å…± {total_before} æ¡ï¼Œå¼€å§‹ DNS éªŒè¯...")

    # æ‰¹é‡éªŒè¯
    valid = []
    batch = []
    for r in rules:
        batch.append(r)
        if len(batch) >= DNS_BATCH_SIZE:
            valid.extend(validate_rules(batch))
            batch = []
    if batch:
        valid.extend(validate_rules(batch))

    valid = sorted(set(valid))
    total_after = len(valid)
    removed_count = total_before - total_after

    # ===== è¿ç»­åˆ é™¤è®¡æ•° =====
    counter = load_delete_counter()
    part_key = f"part_{part:02d}"
    delete_ratio = removed_count / total_before if total_before else 0

    if delete_ratio > 0.10:  # åˆ é™¤æ¯”ä¾‹è¶…è¿‡ 10% æ‰è®¡æ¬¡æ•°
        counter[part_key] = counter.get(part_key, 0) + 1
    else:
        counter[part_key] = 0  # é‡ç½®
    save_delete_counter(counter)

    # ç¬¬å››æ¬¡è¿ç»­åˆ é™¤æ‰ç”Ÿæ•ˆ
    if counter[part_key] < DELETE_CONFIRM_TIMES:
        print(f"âš  åˆ†ç‰‡ {part:02d} åˆ é™¤ {removed_count} æ¡ï¼Œä½†æœªè¾¾åˆ° {DELETE_CONFIRM_TIMES} æ¬¡ç¡®è®¤ï¼Œä¸å†™å…¥ã€‚")
        valid = rules  # ä¿ç•™åŸå†…å®¹
        removed_count = 0

    # å†™å…¥ç»“æœæ–‡ä»¶
    out_file = os.path.join(DIST_DIR, f"validated_part_{part:02d}.txt")
    with open(out_file, "w", encoding="utf-8") as f:
        f.write("\n".join(valid))

    # ç»Ÿè®¡æ–°å¢
    # æ–°å¢ = dist æœ€æ–° - merged_rules
    merged = []
    if os.path.exists(MERGED_RULE):
        with open(MERGED_RULE, "r", encoding="utf-8") as f:
            merged = set(i.strip() for i in f if i.strip())

    old_rules = []
    if os.path.exists(out_file):
        with open(out_file, "r", encoding="utf-8") as f:
            old_rules = set(i.strip() for i in f if i.strip())

    added_count = len(old_rules - merged)

    print(f"âœ… åˆ†ç‰‡ {part:02d} å®Œæˆ â†’ æ€» {len(valid)}, æ–°å¢ {added_count}, åˆ é™¤ {removed_count}")

    # âœ… å†™å…¥ GITHUB_ENV è®© workflow è·å–ç»Ÿè®¡
    if "GITHUB_ENV" in os.environ:
        with open(os.environ["GITHUB_ENV"], "a") as env:
            env.write(f"PART_STATS=æ€»æ•° {len(valid)}, æ–°å¢ {added_count}, åˆ é™¤ {removed_count}\n")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--part", type=int, help="éªŒè¯åˆ†ç‰‡ 1 ~ 16")
    parser.add_argument("--force-update", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½å¹¶åˆ†ç‰‡")
    args = parser.parse_args()

    if args.force-update:
        merge_rules()
        print("âœ… å·²å¼ºåˆ¶æ›´æ–°è§„åˆ™æº & åˆ†ç‰‡")
        return

    if args.part:
        process_part(args.part)
    else:
        print("âŒ å¿…é¡»æŒ‡å®š --part æˆ– --force-update")

if __name__ == "__main__":
    main()
