import os
import requests
import dns.resolver
import concurrent.futures
from datetime import datetime, timezone
import time
import socket
import argparse

# ===================== é…ç½® =====================
URLS_FILE = "urls.txt"
OUTPUT_DIR = "dist"
TMP_DIR = "tmp"
PARTS = 16
DNS_WORKERS = 10
DNS_BATCH_SIZE = 500
DNS_TIMEOUT = 1.5
BATCH_SLEEP = 0.5
RETRY_ON_FAIL = True

socket.setdefaulttimeout(DNS_TIMEOUT)

resolver = dns.resolver.Resolver()
resolver.nameservers = ["1.1.1.1", "8.8.8.8", "9.9.9.9"]
resolver.timeout = DNS_TIMEOUT
resolver.lifetime = DNS_TIMEOUT

# ===================== å‡½æ•° =====================
def safe_fetch(url):
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        return r.text.splitlines()
    except Exception:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼š{url}")
        return []

def clean_rule(line):
    l = line.strip()
    if not l or l.startswith("#") or l.startswith("!"):
        return None
    return l

def extract_domain(rule):
    return rule.lstrip("|").lstrip(".").split("^")[0].strip()

def is_valid_domain(domain):
    try:
        resolver.resolve(domain, "A")
        return True
    except Exception:
        return False

def check_rule(rule):
    try:
        domain = extract_domain(rule)
        if is_valid_domain(domain):
            return rule, None
        else:
            return None, domain
    except Exception:
        return None, extract_domain(rule)

def validate_batch(rules):
    valid_rules = []
    failed_domains = []
    for i in range(0, len(rules), DNS_BATCH_SIZE):
        batch = rules[i:i+DNS_BATCH_SIZE]
        with concurrent.futures.ThreadPoolExecutor(max_workers=DNS_WORKERS) as ex:
            results = list(ex.map(check_rule, batch))
        for r, f in results:
            if r:
                valid_rules.append(r)
            if f:
                failed_domains.append(f)
        print(f"âœ… å·²éªŒè¯ {min(i+DNS_BATCH_SIZE, len(rules)):,}/{len(rules):,} æ¡ï¼Œæœ¬æ‰¹æœ‰æ•ˆ {sum(1 for r,f in results if r):,} æ¡")
        time.sleep(BATCH_SLEEP)
    return valid_rules, failed_domains

# ===================== ä¸»å‡½æ•° =====================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--part", type=int, help="æ‰‹åŠ¨éªŒè¯æŒ‡å®šåˆ†ç‰‡ 0~15")
    args = parser.parse_args()

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(TMP_DIR, exist_ok=True)

    part_files = [os.path.join(TMP_DIR, f"part_{i:02d}.txt") for i in range(PARTS)]
    validated_files = [os.path.join(TMP_DIR, f"validated_{i:02d}.txt") for i in range(PARTS)]
    failed_files = [os.path.join(TMP_DIR, f"failed_{i:02d}.txt") for i in range(PARTS)]
    final_output = os.path.join(OUTPUT_DIR, "blocklist_valid.txt")

    # ===================== é¦–æ¬¡åˆ‡ç‰‡ =====================
    if not os.path.exists(part_files[0]):
        if not os.path.exists(URLS_FILE):
            print("âŒ æœªæ‰¾åˆ° urls.txt")
            return
        with open(URLS_FILE, "r", encoding="utf-8") as f:
            urls = [x.strip() for x in f if x.strip() and not x.startswith("#")]

        print("ğŸ“¥ ä¸‹è½½è§„åˆ™æº...")
        all_rules = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as ex:
            for lines in ex.map(safe_fetch, urls):
                all_rules.extend(lines)

        # å»æ³¨é‡Š & å»é‡
        cleaned = list(dict.fromkeys([clean_rule(x) for x in all_rules if clean_rule(x)]))
        total = len(cleaned)
        print(f"âœ… å»é‡åæ€»è®¡ï¼š{total:,} æ¡")

        chunk = total // PARTS
        for idx in range(PARTS):
            start = idx * chunk
            end = None if idx == PARTS - 1 else (idx + 1) * chunk
            part_data = cleaned[start:end]
            with open(part_files[idx], "w", encoding="utf-8") as f:
                f.write("\n".join(part_data))
            print(f"ğŸ“„ åˆ†ç‰‡ {idx+1} ä¿å­˜ {len(part_data):,} æ¡è§„åˆ™ â†’ {part_files[idx]}")
            print("å‰ 10 æ¡ç¤ºä¾‹ï¼š", part_data[:10])

    # ===================== ç¡®å®šå¤„ç†åˆ†ç‰‡ =====================
    if args.part is not None:
        part_index = args.part
    else:
        now = datetime.now(timezone.utc)
        minute = now.hour * 60 + now.minute
        part_index = (minute // 25) % PARTS  # æ¯ 25 åˆ†é’Ÿè½®æ›¿ä¸€æ¬¡

    target_part = part_files[part_index]
    target_validated = validated_files[part_index]
    target_failed = failed_files[part_index]

    if not os.path.exists(target_part):
        print(f"âš ï¸ åˆ†ç‰‡ {part_index} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return

    with open(target_part, "r", encoding="utf-8") as f:
        rules = [x.strip() for x in f if x.strip()]
    total_rules = len(rules)
    print(f"â± å½“å‰å¤„ç†åˆ†ç‰‡ï¼š{target_part}, æ€»è§„åˆ™ {total_rules:,} æ¡")
    print("å‰ 10 æ¡è§„åˆ™ç¤ºä¾‹ï¼š", rules[:10])

    # ===================== DNS éªŒè¯ =====================
    valid_rules, failed_domains = validate_batch(rules)

    # è‡ªåŠ¨é‡è¯•å¤±è´¥åˆ†ç‰‡ä¸€æ¬¡
    if RETRY_ON_FAIL and failed_domains:
        print(f"ğŸ”„ é‡è¯•å¤±è´¥åŸŸå {len(failed_domains):,} æ¡")
        time.sleep(2)
        retry_valid, retry_failed = validate_batch(failed_domains)
        valid_rules.extend(retry_valid)
        failed_domains = retry_failed

    # ä¿å­˜éªŒè¯ç»“æœ
    with open(target_validated, "w", encoding="utf-8") as f:
        f.write("\n".join(valid_rules))
    with open(target_failed, "w", encoding="utf-8") as f:
        f.write("\n".join(failed_domains))

    # è¾“å‡ºåˆ†ç‰‡ summary
    success_count = len(valid_rules)
    fail_count = len(failed_domains)
    success_rate = success_count / total_rules * 100 if total_rules else 0
    print(f"\nğŸ¯ åˆ†ç‰‡ {part_index+1}/{PARTS} Summary:")
    print(f"   æ€»è§„åˆ™: {total_rules:,}")
    print(f"   æœ‰æ•ˆ: {success_count:,}")
    print(f"   å¤±è´¥: {fail_count:,}")
    print(f"   æˆåŠŸç‡: {success_rate:.2f}%\n")

    # åˆå¹¶æ‰€æœ‰åˆ†ç‰‡éªŒè¯ç»“æœ
    all_valid = []
    for vf in validated_files:
        if os.path.exists(vf):
            with open(vf, "r", encoding="utf-8") as f:
                all_valid.extend([line.strip() for line in f if line.strip()])
    all_valid = list(dict.fromkeys(all_valid))
    with open(final_output, "w", encoding="utf-8") as f:
        f.write("\n".join(all_valid))
    print(f"ğŸ¯ æœ€ç»ˆæœ‰æ•ˆè§„åˆ™ç”Ÿæˆï¼š{final_output} å…± {len(all_valid):,} æ¡")
    print("âœ… æœ¬æ¬¡æ‰§è¡Œå®Œæˆï¼Œæ— é”™è¯¯")

if __name__ == "__main__":
    main()
