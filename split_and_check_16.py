#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import json
import requests
import dns.resolver
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# -----------------------------
# é…ç½®
# -----------------------------
URLS_TXT = "urls.txt"
TMP_DIR = "tmp"
DIST_DIR = "dist"
MERGED_FILE = "merged_rules.txt"
DELETE_COUNTER_FILE = os.path.join(DIST_DIR, "delete_counter.json")
VALIDATED_PREFIX = os.path.join(DIST_DIR, "validated_part_")
WORKERS = 50  # DNS å¹¶å‘
PER_PART = 5000  # æ¯ç‰‡æ•°é‡å¯è°ƒ


# âœ… è¾…åŠ©å‡½æ•°ï¼šè¯»å– delete_counter.json
def load_delete_counter():
    if os.path.exists(DELETE_COUNTER_FILE):
        with open(DELETE_COUNTER_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


# âœ… å†™å› delete_counter.json
def save_delete_counter(counter):
    with open(DELETE_COUNTER_FILE, "w", encoding="utf-8") as f:
        json.dump(counter, f, ensure_ascii=False, indent=2)


# âœ… ä¸‹è½½å¹¶åˆå¹¶æ‰€æœ‰è§„åˆ™ï¼ˆæ–°å¢ HOSTS è½¬æ¢é€»è¾‘ï¼‰
def download_all_sources():
    if not os.path.exists(URLS_TXT):
        print("âŒ æœªæ‰¾åˆ° urls.txtï¼Œæ— æ³•ç»§ç»­")
        return False

    print("ğŸ“¥ æ­£åœ¨ä¸‹è½½è§„åˆ™æº...")
    merged = set()

    with open(URLS_TXT, "r", encoding="utf-8") as f:
        urls = [u.strip() for u in f if u.strip()]

    for url in urls:
        print(f"ğŸŒ æ­£åœ¨è·å–ï¼š{url}")
        try:
            r = requests.get(url, timeout=15)
            r.raise_for_status()

            for raw in r.text.splitlines():
                line = raw.strip()
                if not line or line.startswith("#"):
                    continue

                # âœ…âœ…âœ…ã€æœ¬æ¬¡æ–°å¢é€»è¾‘ã€‘HOSTS â†’ AdGuard è½¬æ¢å¼€å§‹
                # æ”¯æŒæ ¼å¼ï¼š
                #   0.0.0.0 domain.com
                #   127.0.0.1  xxx.net
                parts = line.split()
                if len(parts) == 2 and parts[0] in ("0.0.0.0", "127.0.0.1"):
                    domain = parts[1].strip()
                    if domain and "." in domain:
                        line = f"||{domain}^"
                # âœ…âœ…âœ…ã€æœ¬æ¬¡æ–°å¢é€»è¾‘ã€‘HOSTS â†’ AdGuard è½¬æ¢ç»“æŸ

                merged.add(line)

        except Exception as e:
            print(f"âš  æ— æ³•ä¸‹è½½ï¼š{url}   åŸå› ï¼š{e}")

    print(f"âœ… ä¸‹è½½å¹¶åˆå¹¶å®Œæˆï¼Œå…± {len(merged)} æ¡è§„åˆ™")

    with open(MERGED_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(merged)))

    return True


# âœ… åˆ‡ç‰‡
def split_to_parts():
    print("ğŸ”ª æ­£åœ¨åˆ‡ç‰‡è§„åˆ™...")
    if not os.path.exists(MERGED_FILE):
        print("âŒ merged_rules.txt ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ‡ç‰‡")
        return False

    with open(MERGED_FILE, "r", encoding="utf-8") as f:
        rules = [r.strip() for r in f if r.strip()]

    os.makedirs(TMP_DIR, exist_ok=True)

    part = 1
    for i in range(0, len(rules), PER_PART):
        part_file = os.path.join(TMP_DIR, f"part_{part:02d}.txt")
        with open(part_file, "w", encoding="utf-8") as f:
            f.write("\n".join(rules[i:i + PER_PART]))
        print(f"âœ… å·²ç”Ÿæˆ {part_file}")
        part += 1

    return True


# âœ… DNS éªŒè¯å•æ¡
def check_domain(rule):
    # åªéªŒè¯ AdGuard åŸŸåç±»è§„åˆ™ï¼š||domain^
    if rule.startswith("||") and rule.endswith("^"):
        domain = rule[2:-1]
    else:
        return True, rule  # ä¿ç•™éåŸŸåè§„åˆ™

    resolver = dns.resolver.Resolver()
    resolver.lifetime = 2
    resolver.timeout = 2
    try:
        resolver.resolve(domain)
        return True, rule
    except:
        return False, rule


# âœ… éªŒè¯æŸä¸ªåˆ†ç‰‡ï¼ˆpartï¼‰
def validate_part(part):
    part_file = os.path.join(TMP_DIR, f"part_{int(part):02d}.txt")
    if not os.path.exists(part_file):
        print(f"âŒ åˆ†ç‰‡ {part_file} ä¸å­˜åœ¨")
        return

    print(f"ğŸš€ å¼€å§‹éªŒè¯åˆ†ç‰‡ {part_file}")
    counter = load_delete_counter()

    with open(part_file, "r", encoding="utf-8") as f:
        rules = [r.strip() for r in f if r.strip()]

    valid = []
    removed = 0

    with ThreadPoolExecutor(max_workers=WORKERS) as executor:
        futures = [executor.submit(check_domain, r) for r in rules]
        for fu in tqdm(as_completed(futures), total=len(futures), desc=f"åˆ†ç‰‡ {part} éªŒè¯ä¸­"):
            ok, rule = fu.result()

            if ok:
                valid.append(rule)
                if rule in counter:
                    counter[rule] = 0  # æˆåŠŸä¸€æ¬¡æ¸…é›¶
            else:
                # è¿ç»­å¤±è´¥è®¡æ•° +1
                counter[rule] = counter.get(rule, 0) + 1
                if counter[rule] < 4:
                    valid.append(rule)
                else:
                    removed += 1  # è¶…è¿‡ 3 æ¬¡æ‰çœŸæ­£åˆ é™¤

    # å†™å›éªŒè¯åçš„ç»“æœæ–‡ä»¶
    out_file = f"{VALIDATED_PREFIX}{int(part):02d}.txt"
    with open(out_file, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(set(valid))))

    save_delete_counter(counter)

    print(f"âœ… åˆ†ç‰‡ {part} éªŒè¯å®Œæˆï¼šå…± {len(rules)} æ¡ â†’ ä¿ç•™ {len(valid)} æ¡ â†’ åˆ é™¤ {removed} æ¡")
    print(f"COMMIT_STATS: åˆ†ç‰‡{part} å…±{len(rules)}æ¡ ä¿ç•™{len(valid)}æ¡ åˆ é™¤{removed}æ¡")


# âœ… ä¸»å‡½æ•°
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--part", type=str, help="æŒ‡å®šéªŒè¯åˆ†ç‰‡ 1~16")
    parser.add_argument("--force-update", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½ + åˆ‡ç‰‡")
    args = parser.parse_args()

    if args.force_update:
        print("ğŸ”„ å¼ºåˆ¶åˆ·æ–°è§„åˆ™æº...")
        download_all_sources()
        split_to_parts()
        print("âœ… å¼ºåˆ¶åˆ·æ–°ç»“æŸ")
    else:
        if args.part:
            validate_part(args.part)
        else:
            print("âš  æœªæä¾›åˆ†ç‰‡å‚æ•°ï¼Œä¹Ÿæœª --force-updateï¼Œè‡ªåŠ¨ä½¿ç”¨åˆ†ç‰‡ 01")
            validate_part("1")
