#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import argparse
import requests
import dns.resolver
from concurrent.futures import ThreadPoolExecutor, as_completed

# -------------------------------------------------
# âœ… é…ç½®åŒºåŸŸ
# -------------------------------------------------
URLS_FILE = "urls.txt"
TMP_DIR = "tmp"
DIST_DIR = "dist"
DELETE_COUNTER_FILE = os.path.join(DIST_DIR, "delete_counter.json")  # âœ… è®¡æ•°ä¿ç•™ï¼Œä¸è¦†ç›–
PARTS = 16
DNS_TIMEOUT = 3
DNS_WORKERS = 60
BATCH = 500

os.makedirs(TMP_DIR, exist_ok=True)
os.makedirs(DIST_DIR, exist_ok=True)

# -------------------------------------------------
# âœ… åŠ è½½/åˆå§‹åŒ–åˆ é™¤è®¡æ•°
# -------------------------------------------------
def load_delete_counter():
    if not os.path.exists(DELETE_COUNTER_FILE):
        return {}
    try:
        with open(DELETE_COUNTER_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {}

def save_delete_counter(counter):
    with open(DELETE_COUNTER_FILE, "w", encoding="utf-8") as f:
        json.dump(counter, f, ensure_ascii=False, indent=2)

# -------------------------------------------------
# âœ… ä¸‹è½½å¹¶åˆå¹¶å…¨éƒ¨è§„åˆ™æº â†’ merged_rules.txt
# -------------------------------------------------
def download_rules():
    merged_file = os.path.join(TMP_DIR, "merged_rules.txt")

    if os.path.exists(merged_file):
        print("âœ… merged_rules.txt å·²å­˜åœ¨ï¼Œä¸é‡æ–°ä¸‹è½½")
        return merged_file

    all_rules = set()
    print("â¬ ä¸‹è½½è§„åˆ™æºâ€¦")

    with open(URLS_FILE, "r", encoding="utf-8") as f:
        for url in f.read().splitlines():
            if not url.strip():
                continue
            try:
                print(f"ğŸŒ æ­£åœ¨ä¸‹è½½ï¼š{url}")
                resp = requests.get(url, timeout=10)
                for line in resp.text.splitlines():
                    line = line.strip()
                    if line and not line.startswith("#"):
                        all_rules.add(line)
            except:
                print(f"âš  ä¸‹è½½å¤±è´¥ï¼š{url}")

    with open(merged_file, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(all_rules)))

    print(f"âœ… åˆå¹¶å®Œæˆï¼Œå…± {len(all_rules)} æ¡")
    return merged_file

# -------------------------------------------------
# âœ… æ‹†åˆ†16ç‰‡ â†’ tmp/part_01.txt ~ part_16.txt
# âœ… âœ… ä¿®å¤é‡ç‚¹ï¼šå¼ºåˆ¶å†™å…¥ï¼Œä¸ä¼šå†å‡ºç° tmp é‡Œæ²¡æœ‰åˆ†ç‰‡
# -------------------------------------------------
def split_parts(merged_file):
    with open(merged_file, "r", encoding="utf-8") as f:
        rules = f.read().splitlines()

    total = len(rules)
    per = max(1, total // PARTS)

    print(f"ğŸ”ª å¼€å§‹æ‹†åˆ†ï¼šæ€» {total} | æ¯ç‰‡çº¦ {per}")

    for i in range(PARTS):
        part_rules = rules[i * per: (i + 1) * per]
        part_file = os.path.join(TMP_DIR, f"part_{i+1:02d}.txt")

        # âœ…ã€æ”¹åŠ¨ã€‘å¼ºåˆ¶ç”Ÿæˆï¼Œä¸ä¾èµ–æ—§æ–‡ä»¶
        with open(part_file, "w", encoding="utf-8") as f:
            f.write("\n".join(part_rules))

        print(f"âœ… ç”Ÿæˆ {part_file} ({len(part_rules)})")

# -------------------------------------------------
# âœ… DNS éªŒè¯
# -------------------------------------------------
def dns_check(domain):
    try:
        dns.resolver.resolve(domain, "A", lifetime=DNS_TIMEOUT)
        return True
    except:
        return False

def validate_part(part_id):
    part_file = os.path.join(TMP_DIR, f"part_{part_id:02d}.txt")
    validated_file = os.path.join(DIST_DIR, f"validated_part_{part_id:02d}.txt")

    if not os.path.exists(part_file):
        print(f"âš  åˆ†ç‰‡ä¸å­˜åœ¨ï¼š{part_file}")
        return

    print(f"â± å¼€å§‹éªŒè¯åˆ†ç‰‡ {part_id}")

    with open(part_file, "r", encoding="utf-8") as f:
        rules = f.read().splitlines()

    delete_counter = load_delete_counter()
    keep = []
    deleted = 0

    with ThreadPoolExecutor(max_workers=DNS_WORKERS) as pool:
        future_map = {pool.submit(dns_check, r): r for r in rules}

        for fut in as_completed(future_map):
            rule = future_map[fut]
            ok = fut.result()

            if ok:
                delete_counter[rule] = 0  # âœ… æˆåŠŸ â†’ é‡ç½®è®¡æ•°
                keep.append(rule)
            else:
                # âœ… å¤±è´¥ â†’ +1
                if rule not in delete_counter:
                    delete_counter[rule] = 4  # âœ… æ–°å¢åˆå§‹å€¼ = 4
                else:
                    delete_counter[rule] += 1

                if delete_counter[rule] >= 4:  # âœ… è¾¾åˆ°é˜ˆå€¼ â†’ çœŸåˆ é™¤
                    deleted += 1
                else:
                    keep.append(rule)

    # âœ… ä¿å­˜æ›´æ–°è®¡æ•°
    save_delete_counter(delete_counter)

    # âœ… å†™å…¥éªŒè¯ç»“æœ
    with open(validated_file, "w", encoding="utf-8") as f:
        f.write("\n".join(keep))

    print(f"âœ… åˆ†ç‰‡ {part_id} å®Œæˆ | ä¿ç•™ {len(keep)} | åˆ é™¤ {deleted}")

# -------------------------------------------------
# âœ… ä¸»å…¥å£
# -------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--part", type=int, default=0, help="ä»…éªŒè¯æŒ‡å®šåˆ†ç‰‡")
    parser.add_argument("--force-download", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½å…¨éƒ¨è§„åˆ™æº")
    args = parser.parse_args()

    merged_file = os.path.join(TMP_DIR, "merged_rules.txt")

    # âœ… å¼ºåˆ¶ä¸‹è½½æˆ–æ–‡ä»¶ä¸å­˜åœ¨ â†’ ä¸‹è½½
    if args.force_download or not os.path.exists(merged_file):
        merged_file = download_rules()
        split_parts(merged_file)

    if args.part:
        validate_part(args.part)
    else:
        for p in range(1, PARTS + 1):
            validate_part(p)

    print("âœ… å…¨éƒ¨åˆ†ç‰‡éªŒè¯ç»“æŸ")
