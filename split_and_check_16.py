import os
import requests
import dns.resolver
import concurrent.futures
import argparse
from datetime import datetime

URLS_FILE = "urls.txt"
OUTPUT_DIR = "tmp"
PARTS = 16
MAX_WORKERS = 80  # DNS å¹¶å‘çº¿ç¨‹æ•°
DNS_BATCH_SIZE = 800  # æ¯æ‰¹éªŒè¯æ•°é‡

resolver = dns.resolver.Resolver()
resolver.timeout = 1.5
resolver.lifetime = 1.5
resolver.nameservers = ["1.1.1.1", "8.8.8.8", "9.9.9.9"]

parser = argparse.ArgumentParser()
parser.add_argument("--part", type=int, help="æ‰‹åŠ¨éªŒè¯æŒ‡å®šåˆ†ç‰‡ 0-15")
args = parser.parse_args()

def safe_fetch(url):
    try:
        print(f"ğŸ“¥ ä¸‹è½½ï¼š{url}")
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        return r.text.splitlines()
    except:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼š{url}")
        return []

def clean_rule(line):
    l = line.strip()
    if not l or l.startswith("#") or l.startswith("!") or l.startswith("||browser") or l.startswith("||cf.") or l.startswith("||dig.") or l.startswith("||lf-") or l.startswith("||rt.") or l.startswith("||*.ip6.arpa"):
        return None
    return l

def extract_domain(rule):
    return rule.lstrip("|").lstrip(".").split("^")[0].strip()

def is_valid_domain(domain):
    try:
        resolver.resolve(domain, "A")
        return True
    except:
        return False

def check_rule(rule):
    domain = extract_domain(rule)
    return rule if is_valid_domain(domain) else None

def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ä¸‹è½½ urls.txt ä¸­æ‰€æœ‰è§„åˆ™
    if not os.path.exists(URLS_FILE):
        print("âŒ æœªæ‰¾åˆ° urls.txt")
        return

    with open(URLS_FILE, "r", encoding="utf-8") as f:
        urls = [x.strip() for x in f if x.strip() and not x.startswith("#")]

    all_rules = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as ex:
        for lines in ex.map(safe_fetch, urls):
            all_rules.extend(lines)

    # å»æ³¨é‡Šã€å»é‡
    cleaned = list(dict.fromkeys([clean_rule(x) for x in all_rules if clean_rule(x)]))
    total = len(cleaned)
    print(f"âœ… å»é‡åæ€»è®¡ï¼š{total:,} æ¡")

    # åˆ‡æˆ 16 ä»½
    chunk = total // PARTS
    part_files = []
    for idx in range(PARTS):
        start = idx * chunk
        end = None if idx == PARTS - 1 else (idx + 1) * chunk
        part_file = os.path.join(OUTPUT_DIR, f"part_{idx+1:02d}.txt")
        part_files.append(part_file)
        with open(part_file, "w", encoding="utf-8") as f:
            f.write("\n".join(cleaned[start:end]))
        print(f"ğŸ“„ åˆ†ç‰‡ {idx+1} ä¿å­˜ {len(cleaned[start:end])} æ¡è§„åˆ™ â†’ {part_file}")
        print(f"å‰ 10 æ¡ç¤ºä¾‹ï¼š {cleaned[start:end][:10]}")

    # ç¡®å®šéªŒè¯å“ªä¸€ä»½
    if args.part is not None:
        part_index = args.part
    else:
        minute = datetime.utcnow().hour * 60 + datetime.utcnow().minute
        part_index = (minute // 90) % PARTS  # æ¯ 1.5 å°æ—¶åˆ‡æ¢ä¸€æ¬¡

    target_file = part_files[part_index]
    with open(target_file, "r", encoding="utf-8") as f:
        rules = f.read().splitlines()

    print(f"â± å½“å‰å¤„ç†åˆ†ç‰‡ï¼š{target_file}, æ€»è§„åˆ™ {len(rules):,} æ¡")
    print(f"å‰ 10 æ¡è§„åˆ™ç¤ºä¾‹ï¼š {rules[:10]}")

    valid = []
    count = 0
    for i in range(0, len(rules), DNS_BATCH_SIZE):
        batch = rules[i:i+DNS_BATCH_SIZE]
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            results = list(ex.map(check_rule, batch))
        batch_valid = [r for r in results if r]
        valid.extend(batch_valid)
        count += len(batch)
        print(f"âœ… å·²éªŒè¯ {count}/{len(rules):,} æ¡ï¼Œæœ¬æ‰¹æœ‰æ•ˆ {len(batch_valid)} æ¡")

    valid_output = os.path.join("dist", "blocklist_valid.txt")
    os.makedirs("dist", exist_ok=True)
    with open(valid_output, "a", encoding="utf-8") as f:
        f.write("\n".join(valid) + "\n")

    print(f"ğŸ¯ æœ¬æ¬¡æœ‰æ•ˆè§„åˆ™è¿½åŠ è‡³ {valid_output}")
    print("âœ… æ‰§è¡Œç»“æŸ")

if __name__ == "__main__":
    main()
