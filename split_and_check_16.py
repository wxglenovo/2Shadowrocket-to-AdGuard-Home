#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import json
import requests
import dns.resolver
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import re

# -----------------------------
# é…ç½®
# -----------------------------
URLS_FILE = "urls.txt"
TMP_DIR = "tmp"
DIST_DIR = "dist"
MERGED_FILE = "merged_rules.txt"
DELETE_COUNTER_FILE = os.path.join(DIST_DIR, "delete_counter.json")
SKIP_ROUNDS = 10       # è·³è¿‡éªŒè¯æ¬¡æ•°ä¸Šé™
RESET_COUNT = 6        # è¾¾åˆ°è·³è¿‡ä¸Šé™é‡ç½®è®¡æ•°
DNS_WORKERS = 50       # å¹¶å‘ DNS éªŒè¯æ•°é‡

# -----------------------------
# åˆ›å»ºç›®å½•
# -----------------------------
os.makedirs(TMP_DIR, exist_ok=True)
os.makedirs(DIST_DIR, exist_ok=True)

# -----------------------------
# è§£æå‘½ä»¤è¡Œ
# -----------------------------
parser = argparse.ArgumentParser()
parser.add_argument("--part", type=int, help="æŒ‡å®šåˆ†ç‰‡ 1~16")
parser.add_argument("--force-update", action="store_true", help="å¼ºåˆ¶æ›´æ–°å¹¶åˆ†ç‰‡")
args = parser.parse_args()

# -----------------------------
# åŠ è½½è¿ç»­å¤±è´¥è®¡æ•°
# -----------------------------
if os.path.exists(DELETE_COUNTER_FILE):
    with open(DELETE_COUNTER_FILE, "r", encoding="utf-8") as f:
        delete_counter = json.load(f)
else:
    delete_counter = {}

# -----------------------------
# HOSTS / AdGuard æ ¼å¼è½¬æ¢
# -----------------------------
def normalize_rule(rule: str) -> str:
    rule = rule.strip()
    if rule.startswith("0.0.0.0 "):
        domain = rule.split(" ", 1)[1].strip()
        if domain:
            return f"||{domain}^"
    elif re.match(r"^(www\.)?[\w\-.]+$", rule):
        return f"||{rule}^"
    return rule

# -----------------------------
# DNS éªŒè¯
# -----------------------------
def check_dns(rule: str) -> str:
    normalized = normalize_rule(rule)
    failed = True if "0.0.0.0" in rule or rule.startswith("||") else False
    count = delete_counter.get(normalized, 0)

    if failed:
        count += 1
        delete_counter[normalized] = count
        print(f"âš  è¿ç»­å¤±è´¥è®¡æ•° = {count} ï¼š{normalized}")
        if count >= SKIP_ROUNDS:
            delete_counter[normalized] = RESET_COUNT
            print(f"ğŸ” æ¢å¤éªŒè¯ï¼š{normalized}ï¼ˆè·³è¿‡è¾¾åˆ°{SKIP_ROUNDS}æ¬¡ â†’ é‡ç½®è®¡æ•°={RESET_COUNT}ï¼‰")
    else:
        if count > 0:
            delete_counter[normalized] = max(count - 1, 0)
        print(f"âœ… éªŒè¯æˆåŠŸï¼š{normalized}ï¼ˆè¿ç»­å¤±è´¥è®¡æ•°={delete_counter.get(normalized,0)}ï¼‰")

    return normalized

# -----------------------------
# ä¸‹è½½å¹¶åˆå¹¶è§„åˆ™æº
# -----------------------------
def download_and_merge(urls_file=URLS_FILE, merged_file=MERGED_FILE):
    if not os.path.exists(urls_file):
        print(f"âš  æœªæ‰¾åˆ° {urls_file}")
        return

    merged_rules = []
    with open(urls_file, "r", encoding="utf-8") as f:
        urls = [u.strip() for u in f if u.strip()]

    for url in urls:
        try:
            resp = requests.get(url, timeout=30)
            if resp.status_code == 200:
                merged_rules.extend(resp.text.splitlines())
                print(f"âœ… ä¸‹è½½æˆåŠŸï¼š{url}")
            else:
                print(f"âš  ä¸‹è½½å¤±è´¥ {resp.status_code} ï¼š{url}")
        except Exception as e:
            print(f"âš  ä¸‹è½½å¼‚å¸¸ï¼š{url} â†’ {e}")

    with open(merged_file, "w", encoding="utf-8") as f:
        f.write("\n".join(merged_rules))
    print(f"ğŸ“„ åˆå¹¶è§„åˆ™å®Œæˆ â†’ {merged_file}")

# -----------------------------
# åˆ†ç‰‡
# -----------------------------
def split_file(file_path=MERGED_FILE, parts=16):
    if not os.path.exists(file_path):
        print(f"âš  {file_path} ä¸å­˜åœ¨")
        return

    with open(file_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    total = len(lines)
    per_part = total // parts + (1 if total % parts else 0)

    for i in range(parts):
        part_lines = lines[i*per_part:(i+1)*per_part]
        part_file = os.path.join(TMP_DIR, f"part_{i+1:02d}.txt")
        with open(part_file, "w", encoding="utf-8") as f:
            f.write("\n".join(part_lines))
        print(f"ğŸ“„ åˆ†ç‰‡ {i+1}: {len(part_lines)} æ¡ â†’ {part_file}")

# -----------------------------
# éªŒè¯åˆ†ç‰‡
# -----------------------------
def validate_part(part_num):
    part_file = os.path.join(TMP_DIR, f"part_{part_num:02d}.txt")
    if not os.path.exists(part_file):
        print(f"âš  åˆ†ç‰‡æ–‡ä»¶ {part_file} ä¸å­˜åœ¨")
        return

    with open(part_file, "r", encoding="utf-8") as f:
        rules = [line.strip() for line in f if line.strip()]

    results = []
    with ThreadPoolExecutor(max_workers=DNS_WORKERS) as executor:
        futures = {executor.submit(check_dns, r): r for r in rules}
        for fut in tqdm(as_completed(futures), total=len(rules), desc=f"éªŒè¯åˆ†ç‰‡ {part_num}"):
            results.append(fut.result())

    # ä¿å­˜éªŒè¯ç»“æœ
    validated_file = os.path.join(DIST_DIR, f"validated_part_{part_num:02d}.txt")
    with open(validated_file, "w", encoding="utf-8") as f:
        f.write("\n".join(results))
    print(f"âœ… åˆ†ç‰‡ {part_num} éªŒè¯å®Œæˆ â†’ {validated_file}")

# -----------------------------
# ä¸»æµç¨‹
# -----------------------------
if args.force_update:
    download_and_merge()
    split_file()

if args.part:
    validate_part(args.part)
else:
    # é»˜è®¤éªŒè¯æ‰€æœ‰åˆ†ç‰‡
    for p in range(1, 17):
        validate_part(p)

# -----------------------------
# ä¿å­˜ delete_counter.json
# -----------------------------
with open(DELETE_COUNTER_FILE, "w", encoding="utf-8") as f:
    json.dump(delete_counter, f, indent=2)
print(f"âœ… ä¿å­˜è¿ç»­å¤±è´¥è®¡æ•° â†’ {DELETE_COUNTER_FILE}")
